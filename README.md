# Qc\_MartialArts\_NN

This project processes pose estimation JSON output (e.g. from AlphaPose), visualizes human skeletons over time as individual frame plots, and compiles them into a final video for analysis of martial arts movements. It also supports selecting frame ranges using a GUI.

---

## üîß Requirements

- Python 3.7+
- OpenCV
- NumPy
- Matplotlib
- Tkinter (built into most Python installations)

---

## üìÅ Folder Setup (IMPORTANT)

Before using the code, make sure the following folders exist (create them if missing):

- `OpenPose_Code/newplots` ‚Äî leave this empty
- `AlphaPose_Code/output_plots` ‚Äî will store frame plots
- `AlphaPose_Code/selected_frames` ‚Äî will store selected frame plots
- `Video_Outputs/` ‚Äî created automatically for final videos

---

## üß† Files Overview

### `main.py`

Entry point. Launches the full process:

1. Opens a GUI to select:
   - JSON file of keypoints
   - Corresponding video
   - Output video name
2. Plots skeletons for each frame using `reader.py`
3. Lets you select a frame range using a second GUI (`frameGUIandSelect.py`)
4. Copies selected plots to a dedicated folder

### `reader.py`

- Draws pose skeletons from AlphaPose-style JSON output.
- Saves one plot per frame to `AlphaPose_Code/output_plots/`.
- Creates a final video from the plots using `videoCreator.py`.

### `frameGUIandSelect.py`

- GUI for entering start and end times (in seconds).
- Converts to frame range and copies corresponding plots to `AlphaPose_Code/selected_frames`.

### `videoCreator.py`

- Turns the plotted PNGs into an `.mp4` video using OpenCV.
- Saves to `Video_Outputs/{your_name}.mp4`.

### `folderclear.py`

- Empties `output_plots/` and `selected_frames/` before processing to avoid mixing runs.

---

## ü•ö Experimental File

- `test2.py` (not currently used in the main pipeline) contains an example neural network directly from the PyTorch documentation. This is for learning purposes and will be adapted in the future.

---

## üß¥ COCO 17 Keypoint Skeleton Edges

These are the connections between keypoints that the plotter uses:

```
(0, 1):  Nose ‚Üí Left Eye
(0, 2):  Nose ‚Üí Right Eye
(1, 3):  Left Eye ‚Üí Left Ear
(2, 4):  Right Eye ‚Üí Right Ear
(0, 5):  Nose ‚Üí Left Shoulder
(0, 6):  Nose ‚Üí Right Shoulder
(5, 7):  Left Shoulder ‚Üí Left Elbow
(7, 9):  Left Elbow ‚Üí Left Wrist
(6, 8):  Right Shoulder ‚Üí Right Elbow
(8, 10): Right Elbow ‚Üí Right Wrist
(5, 11): Left Shoulder ‚Üí Left Hip
(6, 12): Right Shoulder ‚Üí Right Hip
(11, 13): Left Hip ‚Üí Left Knee
(13, 15): Left Knee ‚Üí Left Ankle
(12, 14): Right Hip ‚Üí Right Knee
(14, 16): Right Knee ‚Üí Right Ankle
```

---

## ‚ñ∂Ô∏è How to Run

1. **Run **``
2. **Select your JSON and video**
3. **Enter a name for the output video**
4. **Wait for plots to be generated**
5. **Input start and end times (in seconds) to select a portion**
6. **Review final output in **``

